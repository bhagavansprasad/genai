{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fcc62d1-673f-4366-9b4e-4ed36f5c7d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export GCP service account key file to take care of authentication.\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/rajan/CREDENTIALS/rtc-ai-20240203-c915144c2c55.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ea91734-a21f-49ad-8f17-9b91d0c66800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import nbformat\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tracers.stdout import elapsed\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "import vertexai\n",
    "import requests\n",
    "from langchain.schema.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dcdf40-bc82-4b24-8563-7bf99ad3d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade --user -q google-cloud-aiplatform \\\n",
    "                                langchain \\\n",
    "                                langchain_google_vertexai \\\n",
    "                                langchain-community \\\n",
    "                                faiss-cpu \\\n",
    "                                nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c9bf12-cecd-4007-be14-2c72aff120e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI API version: 1.65.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize vertex AI\n",
    "import vertexai\n",
    "PROJECT_ID = \"rtc-ai-20240203\"      # Replace this with your GCP project id.\n",
    "LOCATION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "print(f\"Vertex AI API version: {vertexai.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7af64f1b-3f01-46ca-8d35-5a54cc9cc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Code LLM\n",
    "code_llm = VertexAI(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    max_output_tokens=2048,\n",
    "    temperature=0.1,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d78a91-5a83-43d5-a015-4118ad94d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f2b599-7f34-4ff2-8ff4-49e477637317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl through git repo and return list of all ipynb files in the repo.\n",
    "def crawl_git_repo(url: str, is_sub_dir: bool, access_token: str):\n",
    "\n",
    "    if is_sub_dir:\n",
    "        api_url = url\n",
    "    else:\n",
    "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
    "\n",
    "    header = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\",\n",
    "        \"Authorization\": f\"token {access_token}\",\n",
    "    }\n",
    "    response = requests.get(api_url, headers=header)\n",
    "\n",
    "    # Check for any request errors\n",
    "    response.raise_for_status()\n",
    "\n",
    "    files = []\n",
    "    contents = response.json()\n",
    "\n",
    "\n",
    "    for item in contents:\n",
    "        if (item[\"type\"] == \"file\"\n",
    "            and (item[\"name\"].endswith(\".py\") or item[\"name\"].endswith(\".ipynb\"))\n",
    "        ):\n",
    "            files.append(item[\"html_url\"])\n",
    "        elif item[\"type\"] == \"dir\" and not item[\"name\"].startswith(\".\"):\n",
    "            sub_files = crawl_git_repo(item[\"url\"], True, access_token)\n",
    "            time.sleep(0.1)         # Wait for 100 milliseconds before next github api call.\n",
    "            files.extend(sub_files)\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35c80699-1280-44cd-b299-475dafed1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "print(GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae711c26-a449-4115-a812-d3f15d5c9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "REPO_URL = \"ibm-et/jupyter-samples\"\n",
    "code_files_urls = crawl_git_repo(REPO_URL, False, GITHUB_TOKEN)\n",
    "\n",
    "# Write code file urls list to a file so that we don't have to download each time.\n",
    "with open(\"code_files_urls.txt\", \"w\") as f:\n",
    "    for url in code_files_urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923cc07a-9e0a-4994-bff8-5d969e84e217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/ibm-et/jupyter-samples/blob/master/airline/Exploration%20of%20Airline%20On-Time%20Performance.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/bluemix-spark-cloudant/1-Streaming-Meetups-to-IBM-Cloudant-using-Spark.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/bluemix-spark-cloudant/2-Reading-Meetups-from-IBM-Cloudant-using-Spark.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/elasticity/Elasticity%20Experiment.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/hacks/IPython%20Parallel%20and%20R.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/hacks/Webserver%20in%20a%20Notebook.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/hacks/instaquery.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/hn/Hacker%20News%20Runner.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/hn/Hacker%20News%20and%20AlchemyAPI.ipynb',\n",
       " 'https://github.com/ibm-et/jupyter-samples/blob/master/index.ipynb']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_files_urls[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c68fd-b2d5-4e78-9224-8022632629c2",
   "metadata": {},
   "source": [
    "# File: my-code-rag-2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a34e1e7-98b3-4fb6-b117-d87416a7e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Extract code from the Jupyter notebook\n",
    "##########################################\n",
    "# Extracts the python code from an ipynb file from github\n",
    "def extract_python_code_from_ipynb(github_url, cell_type=\"code\"):\n",
    "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
    "        \"/blob/\", \"/\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(raw_url)\n",
    "    response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "    notebook_content = response.text\n",
    "\n",
    "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
    "\n",
    "    python_code = None\n",
    "\n",
    "    if hasattr(notebook, 'worksheets'):\n",
    "        cells = notebook.worksheets[0].cells\n",
    "    else:\n",
    "        cells = notebook.cells\n",
    "\n",
    "    for cell in cells:\n",
    "        if cell.cell_type == cell_type:\n",
    "\n",
    "            if hasattr(cell, 'input'):\n",
    "                code = cell.input\n",
    "            else:\n",
    "                code = cell.source\n",
    "\n",
    "            if not python_code:\n",
    "                python_code = code\n",
    "            else:\n",
    "                python_code += \"\\n\" + code\n",
    "\n",
    "    return python_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07b4f45-5e62-4f1b-a4cf-aba000ca7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file count is 23\n"
     ]
    }
   ],
   "source": [
    "# Read the file urls from text file.\n",
    "with open(\"code_files_urls.txt\") as f:\n",
    "    code_files_urls = f.read().splitlines()\n",
    "print(f\"Total file count is {len(code_files_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1622b1b-fcd3-4e04-a074-78d766e1b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_strings = []\n",
    "\n",
    "for i in range(0, len(code_files_urls)):\n",
    "    if code_files_urls[i].endswith(\"ipynb\"):\n",
    "        content = extract_python_code_from_ipynb(code_files_urls[i], \"code\")\n",
    "\n",
    "        if content is not None:\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"url\": code_files_urls[i],\n",
    "                    \"file_index\": i\n",
    "                }\n",
    "            )\n",
    "\n",
    "            code_strings.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba4d29c7-e44e-42f1-af39-de996508994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'url': 'https://github.com/ibm-et/jupyter-samples/blob/master/airline/Exploration%20of%20Airline%20On-Time%20Performance.ipynb', 'file_index': 0}, page_content='!pip install cloudant\\nimport cloudant\\naccount = cloudant.Account(\\'parente\\')\\ndatabase = account.database(\\'rita_transtats_2014_06\\')\\ndatabase.get().json()\\nitems = []\\nfor i, item in enumerate(database.all_docs(params={\\'include_docs\\' : True})):\\n    if i > 1: break\\n    items.append(item)\\nprint items\\nimport pandas\\npandas.DataFrame([item[\\'doc\\'] for item in items])\\ncolumns = [u\\'FL_DATE\\', u\\'ORIGIN_STATE_ABR\\', u\\'DEST_STATE_ABR\\', u\\'ARR_DEL15\\', u\\'ARR_DELAY_NEW\\', u\\'DEP_DEL15\\', u\\'DEP_DELAY_NEW\\', u\\'DISTANCE\\', u\\'DISTANCE_GROUP\\',]\\n%%time\\ndfs = []\\nbuff = []\\nfor i, item in enumerate(database.all_docs(params={\\'include_docs\\' : True})):\\n    buff.append(item[\\'doc\\'])\\n    if i > 0 and i % 20000 == 0:\\n        print \\'Processed #{}\\'.format(i)\\n        df = pandas.DataFrame(buff, columns=columns)\\n        dfs.append(df)\\n        buff = []\\n# don\\'t forget the leftovers\\ndf = pandas.DataFrame(buff, columns=columns)\\ndfs.append(df)\\ndf = pandas.concat(dfs)\\nassert len(df) == database.get().json()[\\'doc_count\\']\\ndel dfs\\ndel buff\\n!free -m\\ndf = df.reset_index(drop=True)\\ndf.DEP_DEL15.value_counts() / len(df)\\ndf.ARR_DEL15.value_counts() / len(df)\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set_palette(\"deep\", desat=.6)\\ncolors = sns.color_palette(\"deep\")\\nsns.set_context(rc={\"figure.figsize\": (18, 5)})\\nby_origin_state = df.groupby(\\'ORIGIN_STATE_ABR\\')\\ndeparture_delay_counts = by_origin_state.DEP_DEL15.sum()\\nby_dest_state = df.groupby(\\'DEST_STATE_ABR\\')\\narrival_delay_counts = by_dest_state.ARR_DEL15.sum()\\ndelay_df = pandas.DataFrame([departure_delay_counts, arrival_delay_counts]).T\\ndelay_df.sort(\\'DEP_DEL15\\', ascending=False).plot(kind=\\'bar\\', title=\\'Number of delayed flights by state\\')\\npct_departure_delay = departure_delay_counts / df.ORIGIN_STATE_ABR.value_counts()\\npct_arrival_delay = arrival_delay_counts / df.DEST_STATE_ABR.value_counts()\\npct_departure_delay.order(ascending=False).plot(kind=\\'bar\\', title=\\'% flights with departure delays by origin state\\')\\npct_arrival_delay.order(ascending=False).plot(kind=\\'bar\\', color=colors[1], title=\\'% flights with arrival delay by destination state\\')\\npct_delay_df = pandas.DataFrame([pct_departure_delay, pct_arrival_delay], index=[\\'PCT_DEP_DEL15\\', \\'PCT_ARR_DEL15\\']).T\\npct_delay_df.sort(\\'PCT_ARR_DEL15\\', ascending=False).plot(kind=\\'bar\\', title=\\'Overlapping % delay plots for comparison\\')\\nfrom __future__ import division\\ndelay_counts_df = df[[\\'ORIGIN_STATE_ABR\\', \\'DEST_STATE_ABR\\', \\'ARR_DEL15\\']].groupby([\\'ORIGIN_STATE_ABR\\', \\'DEST_STATE_ABR\\']).sum()\\ndelay_counts_df.head()\\nsupport = (delay_counts_df / len(df))\\nsupport.head()\\nsupport = support.unstack()\\nsupport.head()\\nsupport = support.T.reset_index(level=0, drop=True).T\\nsupport.head()\\nimport numpy as np\\ndef asymmatplot(plotmat, names=None, cmap=\"Greys\", cmap_range=None, ax=None, **kwargs):\\n    \\'\\'\\'\\n    Plot an asymmetric matrix with colormap and statistic values. A modification of the\\n    symmatplot() function in Seaborn to show the upper-half of the matrix.\\n    \\n    See https://github.com/mwaskom/seaborn/blob/master/seaborn/linearmodels.py for the original.\\n    \\'\\'\\'\\n    if ax is None:\\n        ax = plt.gca()\\n\\n    nvars = len(plotmat)\\n\\n    if cmap_range is None:\\n        vmax = np.nanmax(plotmat) * 1.15\\n        vmin = np.nanmin(plotmat) * 1.15\\n    elif len(cmap_range) == 2:\\n        vmin, vmax = cmap_range\\n    else:\\n        raise ValueError(\"cmap_range argument not understood\")\\n\\n    mat_img = ax.matshow(plotmat, cmap=cmap, vmin=vmin, vmax=vmax, **kwargs)\\n\\n    plt.colorbar(mat_img, shrink=.75)\\n \\n    ax.xaxis.set_ticks_position(\"bottom\")\\n    ax.set_xticklabels(names, rotation=90)\\n    ax.set_yticklabels(names)\\n\\n    minor_ticks = np.linspace(-.5, nvars - 1.5, nvars)\\n    ax.set_xticks(minor_ticks, True)\\n    ax.set_yticks(minor_ticks, True)\\n    major_ticks = np.linspace(0, nvars - 1, nvars)\\n    ax.set_xticks(major_ticks)\\n    ax.set_yticks(major_ticks)\\n    ax.grid(False, which=\"major\")\\n    ax.grid(True, which=\"minor\", linestyle=\"-\")\\n\\n    return ax\\nfig, ax = plt.subplots(figsize=(18,18))\\nasymmatplot(support, names=support.columns, ax=ax, cmap=\\'OrRd\\')\\ntrip_counts_df = df[[\\'ORIGIN_STATE_ABR\\', \\'DEST_STATE_ABR\\', \\'FL_DATE\\']].groupby([\\'ORIGIN_STATE_ABR\\', \\'DEST_STATE_ABR\\']).count()\\ndelay_counts_df = delay_counts_df.rename_axis({\\'ARR_DEL15\\' : \\'COUNTS\\'}, axis=1)\\ntrip_counts_df = trip_counts_df.rename_axis({\\'FL_DATE\\' : \\'COUNTS\\'}, axis=1)\\nmat = (delay_counts_df / trip_counts_df).unstack().T.reset_index(level=0, drop=True).T\\nfig, ax = plt.subplots(figsize=(18,18))\\nasymmatplot(mat, names=mat.columns, ax=ax, cmap=\\'OrRd\\', cmap_range=(0., 1.0))\\nprint delay_counts_df.loc[\\'RI\\', \\'CO\\']\\nprint trip_counts_df.loc[\\'RI\\', \\'CO\\']\\nfig, ax = plt.subplots(figsize=(18,10))\\nsns.boxplot(df.ARR_DELAY_NEW, df.FL_DATE, ax=ax)\\nfig.autofmt_xdate()\\nfig, ax = plt.subplots(figsize=(18,10))\\nsns.boxplot(df.ARR_DELAY_NEW, df.FL_DATE, ax=ax, showfliers=False)\\nfig.autofmt_xdate()\\n!cal 6 2014')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cc2386f-e298-4270-b6e3-93e9b469f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "Waiting\n",
      "..."
     ]
    }
   ],
   "source": [
    "# Chunk code strings\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(code_strings)\n",
    "print(len(texts))\n",
    "\n",
    "\n",
    "# Rate Limiter function\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield       # Request making happens here\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "# Initialize Embeddings API\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "    model_name: str\n",
    "    \n",
    "    def embed_documents(\n",
    "        self, texts: List[str], batch_size: int = 0\n",
    "    ) -> List[List[float]]:\n",
    "        \n",
    "        # setup rate limiter\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        docs = list(texts)\n",
    "        \n",
    "        while docs:\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch : ]\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "        \n",
    "        return [r.values for r in results]\n",
    "\n",
    "\n",
    "\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute = 100,\n",
    "    num_instances_per_batch = 5,\n",
    "    model_name = \"textembedding-gecko@latest\"\n",
    ")\n",
    "\n",
    "# Create Index from embedded code chunks\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",       # Other option is \"mmr\"\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4922816-5b2e-46ff-9c4d-55b3ff43644f",
   "metadata": {},
   "source": [
    "# Lets ask a question using RAG approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4903fdfd-c7fb-4f49-9714-95d5c7c02ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG template\n",
    "prompt_RAG = \"\"\"\n",
    "    You are a proficient python developer. Respond with the syntactically correct code for to the question below. Make sure you follow these rules:\n",
    "    1. Use context to understand the APIs and how to use it & apply.\n",
    "    2. Do not add license information to the output code.\n",
    "    3. Do not include Colab code in the output.\n",
    "    4. Ensure all the requirements in the question are met.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Helpful Response :\n",
    "    \"\"\"\n",
    "\n",
    "prompt_RAG_template = PromptTemplate(\n",
    "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_llm(\n",
    "    llm=code_llm,\n",
    "    prompt=prompt_RAG_template,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fbbc9ef-3dbe-4028-9811-4559ee9969d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from langchain.llms import VertexAI\n",
      "from typing import Any\n",
      "\n",
      "def predict_with_vertex_ai(prompt: str) -> Any:\n",
      "    \"\"\"Takes a prompt and predicts using langchain.llms interface with Vertex AI text-bison model.\n",
      "\n",
      "    Args:\n",
      "        prompt: The input prompt for the model.\n",
      "\n",
      "    Returns:\n",
      "        The response from the Vertex AI text-bison model.\n",
      "    \"\"\"\n",
      "    llm = VertexAI(model_name=\"text-bison@001\")\n",
      "    return llm(prompt)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Create a Python function that takes a prompt and predicts using langchain.llms interface with Vertex AI text-bison model\"\n",
    "results = qa_chain.invoke(input={\"query\": user_question})\n",
    "print(results[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90d374f-61e3-401a-ab5b-fc2c3c713972",
   "metadata": {},
   "source": [
    "# Generate python code using LLM (without RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b429907-1343-4261-b4b4-3eec10ca3b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_zero_shot = \"\"\"\n",
    "    You are a proficient python developer. Respond with the syntactically correct & concise code for to the question below.\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Output Code :\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a886f50-33fc-4c29-bfd9-062efdf21c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from langchain.llms import VertexAI\n",
      "from typing import Optional\n",
      "\n",
      "def predict_with_vertex_ai(prompt: str, \n",
      "                           model_name: Optional[str] = \"text-bison@001\", \n",
      "                           temperature: Optional[float] = 0.7, \n",
      "                           max_output_tokens: Optional[int] = 256) -> str:\n",
      "  \"\"\"\n",
      "  Predicts text using a Vertex AI text generation model.\n",
      "\n",
      "  Args:\n",
      "      prompt: The input prompt for text generation.\n",
      "      model_name: The name of the Vertex AI text generation model to use. \n",
      "                  Defaults to \"text-bison@001\".\n",
      "      temperature: Controls the randomness of the generated text. Higher values \n",
      "                   result in more random text. Defaults to 0.7.\n",
      "      max_output_tokens: The maximum number of tokens to generate in the output. \n",
      "                         Defaults to 256.\n",
      "\n",
      "  Returns:\n",
      "      The generated text.\n",
      "  \"\"\"\n",
      "\n",
      "  llm = VertexAI(model_name=model_name, \n",
      "                 temperature=temperature, \n",
      "                 max_output_tokens=max_output_tokens)\n",
      "  \n",
      "  response = llm(prompt)\n",
      "  return response\n",
      "\n",
      "# Example usage:\n",
      "prompt = \"Write a short story about a cat who goes on an adventure.\"\n",
      "generated_text = predict_with_vertex_ai(prompt)\n",
      "print(generated_text)\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import necessary libraries:**\n",
      "   - `langchain.llms`: Imports the `VertexAI` class for interacting with Vertex AI models.\n",
      "   - `typing.Optional`: Allows for optional function parameters.\n",
      "\n",
      "2. **Define the `predict_with_vertex_ai` function:**\n",
      "   - Takes the following arguments:\n",
      "     - `prompt`: The input prompt for text generation (required).\n",
      "     - `model_name`: The name of the Vertex AI text generation model. Defaults to \"text-bison@001\".\n",
      "     - `temperature`: Controls the randomness of the generated text. Defaults to 0.7.\n",
      "     - `max_output_tokens`: Limits the length of the generated text. Defaults to 256 tokens.\n",
      "   - Creates a `VertexAI` instance using the provided parameters.\n",
      "   - Calls the `llm` object with the prompt to generate text.\n",
      "   - Returns the generated text.\n",
      "\n",
      "3. **Example usage:**\n",
      "   - Sets a sample prompt.\n",
      "   - Calls the `predict_with_vertex_ai` function with the prompt.\n",
      "   - Prints the generated text.\n",
      "\n",
      "**Before running the code:**\n",
      "\n",
      "- **Install required packages:** `pip install langchain google-cloud-aiplatform`\n",
      "- **Set up Google Cloud project and authentication:** Follow the instructions at [https://cloud.google.com/docs/authentication/getting-started](https://cloud.google.com/docs/authentication/getting-started) to set up your Google Cloud project and authenticate your application.\n",
      "\n",
      "This code provides a basic example of using Langchain to access and utilize Vertex AI's text generation capabilities. You can modify the parameters and prompt to experiment with different models and generate various creative text formats. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = code_llm.invoke(input=user_question, max_output_tokens=2048, temperature=0.1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e959d2-0792-407e-8fea-bd8085557d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
